{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89fd54d",
   "metadata": {},
   "source": [
    "## Task: Convert the sentences in the variable sents to vectorized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e928c2b",
   "metadata": {},
   "source": [
    "### Import Count Vectorizer from sklearn.feature_extraction.text and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "348f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1caa01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'The cat sat on the mat',\n",
    "    'The dog is barking at the cat',\n",
    "    'The cat and the dog are friends',\n",
    "    'A quick brown fox jumps over the lazy dog'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54420693",
   "metadata": {},
   "source": [
    "## Approach 1: Using default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "799dccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "521bee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(documents)\n",
    "X_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "016dc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_array, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d5f40",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements, depending on the context and the task at hand. Tokenization is a fundamental step in many natural language processing (NLP) tasks because it allows machines to understand and process human language.\n",
    "\n",
    "Note that here, we have tokenized a sentence by breaking it into individual words.\n",
    "\n",
    "Now, how do we capture the information in seqence of words? Example \"brown fox\" or \"quick brown fox\"?\n",
    "\n",
    "We can use Ngrams in order to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7434782",
   "metadata": {},
   "source": [
    "## Approach 2: Add n_gram range in the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "58bfb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f1cfe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(documents)\n",
    "X_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6718f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_array, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f0e26",
   "metadata": {},
   "source": [
    "In order to reduce number of features, we can add stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192174d3",
   "metadata": {},
   "source": [
    "## Approach 3: Add stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ac407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "040e51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(documents)\n",
    "X_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8324bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_array, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa76dd0",
   "metadata": {},
   "source": [
    "## Approach 4: Add custom pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3a11a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7262f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f443a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom pre-processing function with stemming\n",
    "def custom_preprocessor(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits using regex\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    #Remove Stopwords\n",
    "    words = [word for word in words if word not in stop]\n",
    "    \n",
    "    # Stem or Lemmatize each word\n",
    "    #stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the stemmed words back into a single string\n",
    "    processed_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "60afcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),stop_words='english',preprocessor=custom_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8ef29415",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(documents)\n",
    "X_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c23f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_array, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5a781548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barking</th>\n",
       "      <th>barking cat</th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>cat</th>\n",
       "      <th>cat dog</th>\n",
       "      <th>cat sat</th>\n",
       "      <th>dog</th>\n",
       "      <th>dog barking</th>\n",
       "      <th>dog friend</th>\n",
       "      <th>...</th>\n",
       "      <th>friend</th>\n",
       "      <th>jump</th>\n",
       "      <th>jump lazy</th>\n",
       "      <th>lazy</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>mat</th>\n",
       "      <th>quick</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sat</th>\n",
       "      <th>sat mat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   barking  barking cat  brown  brown fox  cat  cat dog  cat sat  dog  \\\n",
       "0        0            0      0          0    1        0        1    0   \n",
       "1        1            1      0          0    1        0        0    1   \n",
       "2        0            0      0          0    1        1        0    1   \n",
       "3        0            0      1          1    0        0        0    1   \n",
       "\n",
       "   dog barking  dog friend  ...  friend  jump  jump lazy  lazy  lazy dog  mat  \\\n",
       "0            0           0  ...       0     0          0     0         0    1   \n",
       "1            1           0  ...       0     0          0     0         0    0   \n",
       "2            0           1  ...       1     0          0     0         0    0   \n",
       "3            0           0  ...       0     1          1     1         1    0   \n",
       "\n",
       "   quick  quick brown  sat  sat mat  \n",
       "0      0            0    1        1  \n",
       "1      0            0    0        0  \n",
       "2      0            0    0        0  \n",
       "3      1            1    0        0  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0218f5",
   "metadata": {},
   "source": [
    "## Now that we understand the basics of Vetorization, Vectorize the documents in the variable document using Tf-IDF vectorizer. Use english stopwords, custom preprocessor and ngram_range of (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d6f3ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(preprocessor=custom_preprocessor,stop_words='english',ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "61b39140",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3284daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X.toarray(),columns=tfidf.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
